---
title: "STAT456 Final Project"
author: "Michelle Harris"
output: pdf_document
date: "2024-04-19"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Reading in the dataset

```{r}
car_price_data <- read.csv("CarPrice_Assignment.csv")
```

```{r}
car_price_data$symboling <- as.character(car_price_data$symboling)
```


#EDA

```{r}
summary(car_price_data)
```


```{r}
library(ggplot2)
pairs(car_price_data[, c('price', 'wheelbase', 'carlength')], col = '#008000')
pairs(car_price_data[, c('price', 'carwidth', 'carheight', 'curbweight')], col = '#4CAF50')
pairs(car_price_data[, c('price', 'enginesize', 'boreratio', 'stroke')], col = '#8BC34A')
pairs(car_price_data[, c('price', 'horsepower', 'compressionratio')], col = '#CDDC39')
pairs(car_price_data[, c('price', 'peakrpm', 'citympg', 'highwaympg')], col = '#FFD700')
```


#EDA Findings

Visually, 'wheelbase', 'carlength', 'carwidth', 'curbweight', 'enginesize', 'horsepower', 'citympg', and 'highwaympg' are associated with 'price.'


#EDA plots based on findings

```{r}
plot(price ~ wheelbase, data = car_price_data)
plot(price ~ carlength, data = car_price_data)
plot(price ~ carwidth, data = car_price_data)
plot(price ~ curbweight, data = car_price_data)
plot(price ~ enginesize, data = car_price_data)
plot(price ~ horsepower, data = car_price_data)
plot(price ~ citympg, data = car_price_data)
plot(price ~ highwaympg, data = car_price_data)
plot(price ~ peakrpm, data = car_price_data)
```

#Train and Test Data

```{r}
library(dplyr)

exclude_vars <- c('car_ID','symboling','CarName','fueltype','aspiration','doornumber','carbody','drivewheel','enginelocation','enginetype','cylindernumber','fuelsystem')  

car_price_data_new <- select(car_price_data, -one_of(exclude_vars))
```


```{r}
train_data <- car_price_data_new[1:165, ]
test_data <- car_price_data_new[166:205, ]

```


#Linear Regression Model

```{r}
linear_model <- lm(price ~ . - car_ID, data = car_price_data)
summary(linear_model)
```
We first started with the linear model with all variables given. We got R-squared value of 0.9762, which indicates that 97.62 of the variability
in the car price is explained by all variables. Notice that p-value is 9.666e-16, which is very close to 0. This p-value tells us that this model 
is statistically significant.

```{r}
#start with linear model containing all continuous variables
linear_model2 <- lm(price ~ ., data = train_data)
predicted <- predict(linear_model2, newdata = test_data)
rmse <- sqrt(mean((test_data$price - predicted)^2))
rmse
summary(linear_model2)
```
We moved onto the second model, where we decided to use only numerical variables to calculate RMSE.
We got the adjusted R-squared of 0.8589, which indicates that 85.89% of the variability in car price is explained
by all numerical variables. The R-squared value has decreased from the first model, and the p-value has also decreased.
Using train and testing data, we got RMSE value of 3224.741.


```{r}
linear_model3 <- lm(price ~ stroke + compressionratio + peakrpm + enginesize + horsepower, data = train_data)
predicted <- predict(linear_model3, newdata = test_data)
rmse <- sqrt(mean((test_data$price - predicted)^2))
rmse
summary(linear_model3)
```
For the last model, we selected five most significant variables: stroke, compressionratio, peakrpm, enginesize, and horsepower.
These variables had the least p-values from the output of the second model. We got the adjusted R-squared of 0.8488, which indicates 
that 84.88% of the variability in car price is explained by all numerical variables. The R-squared value has increased from the first model, 
while the p-value has stayed the same. Using train and testing data, we got RMSE value of 3353.8, which is greater than the RMSE value from model 2.

#Stepwise Regression

```{r}
initial_model <- lm(price ~ ., data = train_data)
#stepwise using aic
stepwise_model_aic <- step(initial_model, direction = "both", trace = 1, k = log(nrow(train_data)), criterion = "aic")
#stepwise using bic
stepwise_model_bic <- step(initial_model, direction = "both", trace = 1, k = log(nrow(train_data)), criterion = "bic")
#stepwise using adjr2
stepwise_model_adjr2 <- step(initial_model, direction = "both", trace = 1, k = log(nrow(train_data)), criterion = "adjr2")
summary(stepwise_model_aic)
summary(stepwise_model_bic)
summary(stepwise_model_adjr2)
```

All of these summaries suggest that the best model is:

price = -41674.1 + 248.872 * wheelbase + 119.839 * enginesize - 2875.852 * stroke
+ 300.989 * compressionratio + 57.279 * horsepower + 3.096 * peakrpm

#Testing the stepwise model

```{r}
stepwise_model_selected <- stepwise_model_aic
predictions <- predict(stepwise_model_selected, newdata = test_data)
rmse <- sqrt(mean((test_data$price - predictions)^2))
rmse
```

We can see that the RMSE is too large. Perhaps outliers affect this?

```{r}
continuous_vars <- c('wheelbase', 'carlength', 'carwidth', 'carheight', 'curbweight', 'enginesize', 'boreratio', 'stroke', 'compressionratio', 'horsepower', 'peakrpm', 'citympg', 'highwaympg')
remove_outliers_threshold <- function(data, threshold = 3) {
  for (var in continuous_vars) {
    data <- data[abs(data[[var]] - mean(data[[var]])) <= threshold * sd(data[[var]]), ]
  }
  return(data)
}
car_price_data_clean <- remove_outliers_threshold(car_price_data)
par(mfrow = c(2, 3))
for (var in continuous_vars) {
  boxplot(car_price_data_clean[[var]], main = paste("Boxplot of", var))
}
```

After outliers were removed, we have 179 observations. Redoing the training and testing data to account for that.

```{r}
#train and test data after outliers were removed
train_data_clean <- car_price_data_clean[1:139, ]
test_data_clean <- car_price_data_clean[140:179, ]
```



```{r}
initial_model <- lm(price ~ wheelbase + carlength + carwidth + carheight + curbweight + enginesize + boreratio + stroke + compressionratio + horsepower + peakrpm + citympg + highwaympg, data = train_data_clean)
#stepwise using aic
stepwise_model_aic <- step(initial_model, direction = "both", trace = 1, k = log(nrow(train_data_clean)), criterion = "aic")
#stepwise using bic
stepwise_model_bic <- step(initial_model, direction = "both", trace = 1, k = log(nrow(train_data_clean)), criterion = "bic")
#stepwise using adjr2
stepwise_model_adjr2 <- step(initial_model, direction = "both", trace = 1, k = log(nrow(train_data_clean)), criterion = "adjr2")
summary(stepwise_model_aic)
summary(stepwise_model_bic)
summary(stepwise_model_adjr2)
```

```{r}
stepwise_model_selected <- stepwise_model_aic
predictions <- predict(stepwise_model_selected, newdata = test_data)
rmse <- sqrt(mean((test_data$price - predictions)^2))
rmse
```

RMSE went down after removing outliers.


#More EDA based on categorical data

```{r}
library(dplyr)
car_price_data_clean <- car_price_data %>%
  mutate(
    bmwx1 = as.integer(CarName == "bmw x1"),
    bmwx3 = as.integer(CarName == "bmw x3"),
    bmwx4 = as.integer(CarName == "bmw x4"),
    bmwx5 = as.integer(CarName == "bmw x5"),
    bmwz4 = as.integer(CarName == "bmw z4"),
    buick_century = as.integer(CarName == "buick century"),
    buick_regal_sc = as.integer(CarName == "buick regal sport coupe (turbo)"),
    peugeot304 = as.integer(CarName == "peugeot 304"),
    peugeot504 = as.integer(CarName == "peugeot 504"),
    peugeot604_sl = as.integer(CarName == "peugeot 604 sl"),
    porsche_panamera = as.integer(CarName == "porsche panamera"),
    porsce_boxter = as.integer(CarName == "porsche boxter"),
    porsche_cayenne = as.integer(CarName == "porsche cayenne"),
    fuelgas = as.integer(fueltype == "gas"),
    cyl4 = as.integer(cylindernumber == "four")
  )
```


```{r}
library(ggplot2)
ggplot(car_price_data_clean, aes(x = enginesize, y = price, color = fueltype)) +
  geom_point() +
  labs(x = "engine size", y = "price", color = "fuel type")
```

A little hard to read, but four cylinders seem to be associated with lower price,

```{r}
library(ggplot2)
ggplot(car_price_data_clean, aes(x = enginesize, y = price, color = cylindernumber)) +
  geom_point() +
  labs(x = "engine size", y = "price", color = "cylinder number")
```


#```{r}
#library(ggplot2)
#ggplot(data = car_price_data, aes(x = enginesize, y = price, fill = CarName)) +
#  geom_tile() +
#  scale_fill_gradient(low = "yellow", high = "red") + 
#  theme_minimal() +
#  labs(x = "engine size", y = "price", title = "Heatmap of engine size against #price by car name")


#Stepwise Regression Regression vs. Multiple Linear Regression Model
Price = -41674.100 + 248.872 * wheelbase + 119.839 * enginesize – 2875.852 * stroke + 300.989 * compressionratio + 57.279 * horsepower + 3.096 * peakrpm

Price = -48703.162 + 153.510 * wheelbase – 35.536 * carlength + 255.082 * carwidth + 114.007 * carheight + 1.136 * curbweight + 123.411 * enginesize – 324.821 * boreratio –2975.049 * stroke + 310.901 * compressionratio + 37.003 * horsepower – 3.233 * peakrpm – 216.214 * citympg + 145.037 * highwaympg

#Conlusion

Q. Which variables significantly predict the price of a car?
A. The  most significant variables are enginesize, stroke, compressionratio, horsepower, and peakrpm. However, the other variables
like wheelbase, carlength, carwidth, carheight, curbweight, boreratio, horsepower, citympg, and highwaympg also contribute to predict the price of a car.

Q. How well do these variables explain car prices?
A. We can conclude that from both models, 85.89% of the variability in car price is explained by all of numerical variables.



